{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07df58-329f-4392-8ceb-130afe0d0b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def segment_lesion_auto(image_rgb, min_lesion_frac=0.005):\n",
    "    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    inv = 255 - gray\n",
    "    _, th = cv2.threshold(inv, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    th = cv2.medianBlur(th, 5)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    th = cv2.morphologyEx(th, cv2.MORPH_OPEN,  kernel, iterations=1)\n",
    "    num, labels = cv2.connectedComponents(th)\n",
    "    \n",
    "    if num > 1:\n",
    "        areas = [np.sum(labels == i) for i in range(1, num)]\n",
    "        max_lbl = 1 + int(np.argmax(areas))\n",
    "        mask = (labels == max_lbl)\n",
    "    else:\n",
    "        mask = th.astype(bool)\n",
    "        \n",
    "    if mask.mean() < min_lesion_frac:\n",
    "        mask[:] = False\n",
    "    return mask\n",
    "\n",
    "def mask_by_region_auto(image_rgb, lesion_ratio=0.6, bg_ratio=0.4, mask_value=0, seed=None):\n",
    "    assert image_rgb.ndim == 3 and image_rgb.shape[2] == 3\n",
    "    H, W, _ = image_rgb.shape\n",
    "    lesion_mask = segment_lesion_auto(image_rgb)\n",
    "    \n",
    "    lesion_idx = np.flatnonzero(lesion_mask.ravel())\n",
    "    bg_idx = np.flatnonzero(~lesion_mask.ravel())\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_lesion = int(min(len(lesion_idx), max(0, round(lesion_ratio * len(lesion_idx)))))\n",
    "    n_bg     = int(min(len(bg_idx),     max(0, round(bg_ratio     * len(bg_idx)))))\n",
    "    \n",
    "    pick_lesion = rng.choice(lesion_idx, size=n_lesion, replace=False) if n_lesion > 0 else np.array([], dtype=int)\n",
    "    pick_bg     = rng.choice(bg_idx,     size=n_bg,     replace=False) if n_bg > 0     else np.array([], dtype=int)\n",
    "    pick_all    = np.concatenate([pick_lesion, pick_bg])\n",
    "    \n",
    "    masked_img = image_rgb.copy()\n",
    "    if isinstance(mask_value, (tuple, list)) and len(mask_value) == 3:\n",
    "        masked_img.reshape(-1, 3)[pick_all] = mask_value\n",
    "    else:\n",
    "        masked_img.reshape(-1, 3)[pick_all] = (mask_value, mask_value, mask_value)\n",
    "    return masked_img\n",
    "\n",
    "class DSMAEDataset(Dataset):\n",
    "    def __init__(self, img_dir, img_size=224):\n",
    "        self.img_size = img_size\n",
    "        self.img_paths = []\n",
    "        exts = {'jpg', 'jpeg', 'png', 'bmp'}\n",
    "        \n",
    "        for root, _, files in os.walk(img_dir):\n",
    "            for file in files:\n",
    "                if file.split('.')[-1].lower() in exts:\n",
    "                    self.img_paths.append(os.path.join(root, file))\n",
    "        \n",
    "        if len(self.img_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {img_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        bgr = cv2.imread(img_path)\n",
    "        \n",
    "        if bgr is None:\n",
    "            return torch.zeros(3, self.img_size, self.img_size), torch.zeros(3, self.img_size, self.img_size)\n",
    "            \n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        rgb = cv2.resize(rgb, (self.img_size, self.img_size))\n",
    "        \n",
    "        masked_img_np = mask_by_region_auto(rgb, lesion_ratio=0.8, bg_ratio=0.1, mask_value=0)\n",
    "        \n",
    "        original_tensor = torch.from_numpy(rgb).permute(2, 0, 1).float() / 255.0\n",
    "        masked_tensor = torch.from_numpy(masked_img_np).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        return masked_tensor, original_tensor\n",
    "\n",
    "class DS_MAE(nn.Module):\n",
    "    def __init__(self, yolo_model_path='yolo11s.pt'):\n",
    "        super(DS_MAE, self).__init__()\n",
    "        full_yolo = YOLO(yolo_model_path)\n",
    "        \n",
    "        # Extract backbone layers (0-9)\n",
    "        original_layers = list(full_yolo.model.model.children())\n",
    "        self.encoder = nn.Sequential(*original_layers[:10])\n",
    "        \n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 224, 224)\n",
    "            enc_out = self.encoder(dummy)\n",
    "            self.enc_channels = enc_out.shape[1] \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.enc_channels, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(True),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.decoder(features)\n",
    "    \n",
    "    def get_backbone(self):\n",
    "        return self.encoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 3\n",
    "    IMAGE_DIR = \"/home/hank52052/Dataset/isic/HAM10000/yolo_format/train\"\n",
    "\n",
    "    model = DS_MAE(\"yolo11s.pt\").to(device)\n",
    "    \n",
    "    if os.path.exists(IMAGE_DIR):\n",
    "        dataset = DSMAEDataset(IMAGE_DIR)\n",
    "        \n",
    "        if len(dataset) > 0:\n",
    "            dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "            scheduler = optim.lr_scheduler.CyclicLR(\n",
    "                optimizer, \n",
    "                base_lr=1e-5, \n",
    "                max_lr=1e-1, \n",
    "                step_size_up=200, \n",
    "                mode='triangular',\n",
    "                cycle_momentum=False\n",
    "            )\n",
    "\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                model.train()\n",
    "                total_loss = 0.0\n",
    "                \n",
    "                for i, (masked, original) in enumerate(dataloader):\n",
    "                    masked, original = masked.to(device), original.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(masked)\n",
    "                    loss = criterion(outputs, original)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "            torch.save(model.get_backbone().state_dict(), \"ds_mae_backbone_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de4197-4d2f-45d4-ae86-314f63acf3a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ds_mae_weights_path = \"ds_mae_backbone_weights.pth\" \n",
    "custom_yaml_path = 'yolo11s-cls-shuffle.yaml'\n",
    "\n",
    "model = YOLO(custom_yaml_path)\n",
    "\n",
    "print(f\"[INFO] Loading DS-MAE pretrained backbone weights from: {ds_mae_weights_path}...\")\n",
    "\n",
    "ds_mae_state_dict = torch.load(ds_mae_weights_path)\n",
    "current_model_dict = model.model.state_dict()\n",
    "new_state_dict = {}\n",
    "\n",
    "for k, v in ds_mae_state_dict.items():\n",
    "    target_key = f\"model.{k}\"\n",
    "    \n",
    "    if target_key in current_model_dict:\n",
    "        if current_model_dict[target_key].shape == v.shape:\n",
    "            new_state_dict[target_key] = v\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping layer due to shape mismatch: {target_key}\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Skipping unknown layer: {target_key}\")\n",
    "\n",
    "model.model.load_state_dict(new_state_dict, strict=False)\n",
    "print(f\"[INFO] Successfully loaded {len(new_state_dict)} layers from DS-MAE backbone.\")\n",
    "\n",
    "results = model.train(\n",
    "    data=\"/home/hank52052/Dataset/isic/HAM10000/Four_Classes\",\n",
    "    project=\"/home/hank52052/code/isic/yolo_training\",\n",
    "    epochs=1200,\n",
    "    imgsz=256,\n",
    "    batch=64,\n",
    "    patience=40,\n",
    "    plots=True,\n",
    "    \n",
    "    # Augmentation Hyperparameters\n",
    "    degrees=0.95, \n",
    "    scale=0.9, \n",
    "    shear=0.8, \n",
    "    flipud=0.9, \n",
    "    fliplr=0.9,\n",
    "    hsv_h=0.8, \n",
    "    hsv_s=0.8, \n",
    "    hsv_v=0.9, \n",
    "    translate=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9517b1-fb95-4efa-938b-4b304cd96149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "custom_yaml_path = 'yolo11s-cls-shuffle.yaml'\n",
    "model = YOLO(custom_yaml_path)\n",
    "\n",
    "stage1_weights_path = \"/home/hank52052/code/isic/yolo_training/train/weights/best.pt\"\n",
    "\n",
    "print(f\"[INFO] Loading Stage 1 pretrained weights from: {stage1_weights_path}...\")\n",
    "\n",
    "try:\n",
    "    stage1_model = YOLO(stage1_weights_path)\n",
    "    model.model.load_state_dict(stage1_model.model.state_dict(), strict=False)\n",
    "    print(\"[INFO] Successfully loaded backbone weights. Classification head initialized for 7-class fine-grained task.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Failed to load Stage 1 weights: {e}\")\n",
    "    print(\"[INFO] Proceeding with random initialization.\")\n",
    "\n",
    "results = model.train(\n",
    "    data=\"/home/hank52052/Dataset/isic/HAM10000/Original_Classes\",\n",
    "    project=\"/home/hank52052/code/isic/yolo_training\",\n",
    "    name=\"seven_classes_finetune\",\n",
    "    epochs=1200,\n",
    "    imgsz=256,\n",
    "    patience=40,\n",
    "    batch=64,\n",
    "    rect=True,\n",
    "    plots=True,\n",
    "    \n",
    "    degrees=0.95, \n",
    "    scale=0.9, \n",
    "    shear=0.8, \n",
    "    flipud=0.9, \n",
    "    fliplr=0.9,\n",
    "    hsv_h=0.8, \n",
    "    hsv_s=0.8, \n",
    "    hsv_v=0.9, \n",
    "    translate=0.8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
